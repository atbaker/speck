version: '3'
services:
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    ipc: host
    command: "--host 0.0.0.0 --port 8000 --api-key ${VLLM_API_KEY} --model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 --enforce-eager --gpu-memory-utilization 0.99 --tensor-parallel-size 1 --max-model-len 10240 --quantization awq --enable-prefix-caching"
    container_name: vllm
    networks:
      - clu_network
    ports:
      - "8000:8000"

  nginx:
    image: nginx:latest
    container_name: nginx-proxy
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
      - /etc/letsencrypt:/etc/letsencrypt
    ports:
      - "443:443"
    networks:
      - clu_network

networks:
  clu_network:
    driver: bridge
